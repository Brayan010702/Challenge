{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba1ec38",
   "metadata": {},
   "source": [
    "# 0. Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c3014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from ultralytics import YOLO\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab12f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ejecuci√≥n local (Mac)\n",
    "DATA_YAML_PATH = \"../data/data.yaml\"\n",
    "\n",
    "assert os.path.exists(DATA_YAML_PATH), f\"data.yaml not found in {DATA_YAML_PATH}\"\n",
    "\n",
    "with open(DATA_YAML_PATH, \"r\") as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "data_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fff5e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = data_cfg.get(\"train\")\n",
    "val_dir   = data_cfg.get(\"val\")\n",
    "test_dir  = data_cfg.get(\"test\", None)\n",
    "class_names = data_cfg.get(\"names\", [])\n",
    "nc = int(data_cfg.get(\"nc\", len(class_names)))\n",
    "\n",
    "print(\"Train images dir:\", train_dir)\n",
    "print(\"Val images dir  :\", val_dir)\n",
    "print(\"Test images dir :\", test_dir)\n",
    "print(\"Classes (nc)     :\", nc)\n",
    "print(\"Classes name:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994b156",
   "metadata": {},
   "source": [
    "# 1. Data Analysis: First Sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4690c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_label_paths(img_dir: str) -> list:\n",
    "    img_paths = []\n",
    "    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"):\n",
    "        img_paths.extend(glob.glob(os.path.join(img_dir, ext)))\n",
    "    label_paths = []\n",
    "    for ip in img_paths:\n",
    "        lp = ip.replace(os.sep + \"images\" + os.sep, os.sep + \"labels\" + os.sep)\n",
    "        lp = os.path.splitext(lp)[0] + \".txt\"\n",
    "        label_paths.append((ip, lp))\n",
    "    return label_paths\n",
    "\n",
    "def read_yolo_labels(label_path: str):\n",
    "    if not os.path.exists(label_path):\n",
    "        return []\n",
    "    rows = []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                cls_id = int(float(parts[0]))\n",
    "                cx, cy, w, h = map(float, parts[1:5])\n",
    "                rows.append((cls_id, cx, cy, w, h))\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af5a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_split(img_dir: str, class_names: list):\n",
    "    pairs = yolo_label_paths(img_dir)\n",
    "    per_class = Counter()\n",
    "    objs_per_image = []\n",
    "    areas = []\n",
    "\n",
    "    for _, lbl_path in pairs:\n",
    "        labels = read_yolo_labels(lbl_path)\n",
    "        objs_per_image.append(len(labels))\n",
    "        for (cls_id, cx, cy, w, h) in labels:\n",
    "            per_class[cls_id] += 1\n",
    "            areas.append(w * h)\n",
    "\n",
    "    df_classes = pd.DataFrame({\n",
    "        \"class_id\": list(per_class.keys()),\n",
    "        \"count\": list(per_class.values())\n",
    "    })\n",
    "    df_classes[\"class_name\"] = df_classes[\"class_id\"].apply(lambda i: class_names[i] if i < len(class_names) else str(i))\n",
    "\n",
    "    df_objs = pd.DataFrame({\"objects_per_image\": objs_per_image})\n",
    "    df_areas = pd.DataFrame({\"bbox_area_norm\": areas})\n",
    "\n",
    "    return df_classes.sort_values(\"count\", ascending=False), df_objs, df_areas\n",
    "\n",
    "df_classes_train, df_objs_train, df_areas_train = analyze_split(train_dir, class_names)\n",
    "df_classes_train.head(), df_objs_train.describe(), df_areas_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c49472",
   "metadata": {},
   "source": [
    "### How is the date distribuited?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "316e13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(df_classes_train[\"class_name\"], df_classes_train[\"count\"])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Object count per class (train)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_objs_train[\"objects_per_image\"], bins=20)\n",
    "plt.title(\"Objects per image (train)\")\n",
    "plt.xlabel(\"# objects\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_areas_train[\"bbox_area_norm\"], bins=30)\n",
    "plt.title(\"Normalized bbox area distribution (train)\")\n",
    "plt.xlabel(\"w*h (normalized)\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c05021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_yolo_bbox(img, bbox, color=(0,255,0), thickness=2):\n",
    "    H, W = img.shape[:2]\n",
    "    cx, cy, bw, bh = bbox\n",
    "    x1 = int((cx - bw/2) * W)\n",
    "    y1 = int((cy - bh/2) * H)\n",
    "    x2 = int((cx + bw/2) * W)\n",
    "    y2 = int((cy + bh/2) * H)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "    return img\n",
    "\n",
    "def visualize_samples(img_dir: str, class_names: list, n=4, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    pairs = yolo_label_paths(img_dir)\n",
    "    sample = rng.sample(pairs, min(n, len(pairs)))\n",
    "    fig, axes = plt.subplots(1, len(sample), figsize=(4*len(sample), 4))\n",
    "    if len(sample) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, (img_path, lbl_path) in zip(axes, sample):\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        labels = read_yolo_labels(lbl_path)\n",
    "        for (cls_id, cx, cy, w, h) in labels:\n",
    "            img = draw_yolo_bbox(img, (cx, cy, w, h))\n",
    "            name = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
    "            ax.text(5, 15, name, color='yellow', bbox=dict(facecolor='black', alpha=0.5))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(Path(img_path).name)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_dir, class_names, n=4, seed=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a7ba8",
   "metadata": {},
   "source": [
    "### What methods would you use to verify the reliability of the labels?\n",
    "\n",
    "To verify the reliability of the labels, I would start by looking directly at a sample of images from each class and checking whether the boxes and classes actually make sense, paying extra attention to the rare categories. This helps catch obvious issues like misaligned boxes, incorrect classes, or missing annotations.\n",
    "\n",
    "I‚Äôd also rely on some basic statistical checks to spot anomalies things like bounding boxes that are way too large or too small, images that seem to have an unusually high number of objects, or class distributions that don‚Äôt match what we would expect. These patterns often reveal systematic labeling mistakes. \n",
    "Another useful step is to train a simple model and review the cases where the model is very confident but contradicts the label.\n",
    "Also, if the dataset was labeled by multiple people, I‚Äôd compare their annotations to see how consistent they were. High agreement suggests reliable labeling, low agreement means we should review those samples.\n",
    "\n",
    "From the current analysis, some red flags already stand out strong class imbalance, extremely rare categories that might lack consistent labeling, and images with a very high number of objects. All of these deserve a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f52b6",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b470c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üîß HYPERPARAMETERS ‚Äî EXPERIMENTAL SETUP\n",
    "# ==============================\n",
    "\n",
    "# TODO: Fill in the hyperparameters based on your dataset analysis.\n",
    "# Justify your choices in the Markdown cell above.\n",
    "\n",
    "EPOCHS = 20     # Adjusted for hardware constraints (Google Colab)\n",
    "IMGSZ  = 640    # Standard YOLO size, good balance for industrial scenes  \n",
    "BATCH  = 16     # Optimized for Google Colab\n",
    "DEVICE = \"cuda\"  # Google Colab GPU acceleration T4\n",
    "\n",
    "# Try YOLO11; if not available use YOLOv8\n",
    "weights_try = [\"yolo11n.pt\", \"yolov8n.pt\"]\n",
    "model = None\n",
    "for w in weights_try:\n",
    "    try:\n",
    "        model = YOLO(w)\n",
    "        print(\"Using:\", w)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {w}: {e}\")\n",
    "\n",
    "assert model is not None, \"Could not load a base model (yolo11n.pt / yolov8n.pt). Install ultralytics and make sure you have an active internet connection to download the weights.\"\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# üöÄ TRAINING ‚Äî BASELINE EXPERIMENT\n",
    "# ==============================\n",
    "# The results object contains metrics, charts, and run directory info.\n",
    "# Feel free to adjust and rerun with different hyperparameters.\n",
    "\n",
    "results = model.train(data=DATA_YAML_PATH, epochs=EPOCHS, imgsz=IMGSZ, batch=BATCH, device=DEVICE)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce46504",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Discussion\n",
    "\n",
    "**1. Why did you choose these hyperparameters?**\n",
    "\n",
    "I chose these hyperparameters based on the dataset size and the hardware I currently have available. With around 20,000 images, I needed a number of epochs that would allow the model to converge well without making the training process excessively long, so I went with 20, thinking about an overnight run. The resolution of 640 works well for industrial scenes where very large and very small objects coexist, and it‚Äôs also the YOLO standard that provides the best balance. The batch size of 8 helps avoid memory issues on Apple Silicon, and using mps allows me to take advantage of GPU acceleration, which noticeably speeds things up compared to CPU.\n",
    "\n",
    "**2. How do they affect training time, GPU/CPU usage, and accuracy?**\n",
    "\n",
    "In practice, training with MPS at 640px and a relatively small batch takes about 6‚Äì7 hours for 20 epochs. If I reduced the batch size even more, training would take longer, and lowering the resolution would speed things up but at the cost of worse performance on small objects. The GPU stays quite active during training (around 70‚Äì90%), memory usage is between 8 and 12 GB, and the CPU is mostly involved only in preprocessing. In terms of accuracy, higher resolutions could help capture finer details, but they significantly slow down training; and increasing the number of epochs might slightly improve mAP while also increasing the risk of overfitting.\n",
    "\n",
    "**3. What would you try differently if you had more time or resources?**\n",
    "\n",
    "With a bit more time, I would try larger models like YOLO11s or YOLO11m, adjust the loss function to compensate for class imbalance, and explore augmentations tailored to the lighting conditions in this dataset. With even more time, I would consider a two-stage training approach (pre-training and fine-tuning), combining models for more robust predictions, incorporating an active learning loop to identify hard cases, and running an automatic hyperparameter search to better optimize the configuration. I would also test TTA to improve inference performance.\n",
    "If I also had more resources especially more powerful hardware I would train at higher resolutions (such as 1024) to improve small-object detection, use larger batch sizes for more stable training, and experiment with bigger architectures like YOLO11l or YOLO11x. I could also run parallel experiments or explore more computationally expensive techniques such as complex ensembles or advanced hyperparameter search methods that would be too slow with my current setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05165920",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20906a2",
   "metadata": {},
   "source": [
    "\n",
    "> üëâ **Task:** Evaluate your trained model using the validation set defined in `data.yaml`.\n",
    "\n",
    "Run the following cell to compute key performance metrics\n",
    "Then, summarize your results and provide your interpretation.\n",
    "\n",
    "**Guidelines for your analysis:**\n",
    "- **Quantitative metrics** \n",
    "- **Error analysis**\n",
    "- **Next steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20320b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Here your model metrics\n",
    "# TODO:\n",
    "# - Run model validation on the dataset below.\n",
    "# - Capture metrics and save a summary to artifacts/metrics_summary.json.\n",
    "# - Optionally, add visual analysis (PR curves, confusion matrix).\n",
    "\n",
    "# Put your model evaluation code here üëá\n",
    "\n",
    "metrics = model.val(data=DATA_YAML_PATH, imgsz=IMGSZ, device=DEVICE)\n",
    "try:\n",
    "    summary = {\n",
    "        \"metrics/mAP50-95(B)\": float(metrics.box.map if hasattr(metrics, \"box\") else getattr(metrics, \"map\", float(\"nan\"))),\n",
    "        \"metrics/mAP50(B)\"   : float(getattr(metrics, \"map50\", float(\"nan\"))),\n",
    "        \"nc\": int(nc),\n",
    "        \"classes\": class_names,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"imgsz\": IMGSZ,\n",
    "    }\n",
    "except Exception as e:\n",
    "    summary = {\"error\": str(e)}\n",
    "    \n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "with open(\"artifacts/metrics_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d460a",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Metrics Interpretation and Analysis\n",
    "\n",
    "1. **Quantitative Summary:**\n",
    "   - What are your `mAP50` and `mAP50-95` values?\n",
    "   - Which classes achieved the highest and lowest detection performance?\n",
    "\n",
    "The model achieved 27.5% mAP50 and 18.8% mAP50-95 after 20 epochs. Although these values are below what we typically expect in more controlled datasets, they make sense given the severe class imbalance and the limited training time due to hardware constraints.\n",
    "Looking at the per-class results, a clear pattern appears: classes with more data such as forklift (24,213 samples) and person (20,480 samples) performed noticeably better, reaching 72.6% and 47.4% mAP50. Classes with a moderate amount of data show mid-range performance, whereas the ones with very few annotations like gloves, traffic light, or van, all under 30 samples essentially failed to learn. The relationship is direct: fewer than 100 samples per class usually leads to little or no meaningful learning.\n",
    "\n",
    "2. **Qualitative Analysis:**\n",
    "   - Describe common failure cases (e.g., small objects missed, overlapping detections, background confusion).\n",
    "   - Were there any label quality issues or inconsistencies you observed?\n",
    "\n",
    "Beyond the metrics, several factors help explain these results. The most obvious is the extreme class imbalance: the gap between the most common class and the rarest one is enormous (over 2,000 to 1). This pushes the model to focus on what it sees all the time while ignoring classes that appear only occasionally.\n",
    "There are also challenges with small object detection, such as license plates, QR codes, and gloves. Many of these objects occupy less than 1% of the image, which is difficult for such a lightweight model trained at 640px resolution.\n",
    "On top of that, training for only 20 epochs was likely not enough for the model to fully converge, especially on underrepresented classes. And after inspecting the dataset, some label inconsistencies were observed, particularly in scenes with many objects or partial occlusions.\n",
    "\n",
    "3. **Improvement Proposals:**\n",
    "   - Suggest at least two improvements (data augmentation, loss tuning, class balancing, etc.).\n",
    "   - How would you validate whether these changes actually help?\n",
    "\n",
    "To improve performance, the first step is addressing class imbalance. This can be done by increasing the frequency of rare classes during training and using Focal Loss, which helps the model focus on harder examples. The goal would be to raise rare-class performance to at least ~15% mAP50.\n",
    "It would also help to train at multiple scales and increase the input resolution to 1024px, which gives the model a better chance to capture small objects. Complementing this with stronger data augmentation for rare classes can reduce false negatives. Using TTA during inference can also improve robustness.\n",
    "Another key improvement is extending training to 80-100 epochs with a learning-rate scheduler like cosine annealing, while enabling early stopping to avoid overfitting. If hardware allows it, switching to a larger model variant (YOLO11s or YOLO11m) could make a big difference.\n",
    "Finally, there are some quick wins worth considering: generating synthetic samples for rare classes, adjusting confidence thresholds per class, or even adding a second detection stage specialized in small objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b895120",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_SHOW_N = 4\n",
    "val_imgs = []\n",
    "for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "    val_imgs.extend(glob.glob(os.path.join(val_dir, ext)))\n",
    "val_imgs = val_imgs[:VAL_SHOW_N]\n",
    "\n",
    "# Force CPU device to avoid MPS/CUDA compatibility issues with downloaded weights\n",
    "pred = model.predict(source=val_imgs, imgsz=IMGSZ, conf=0.25, device='cpu')\n",
    "# Mostrar con matplotlib (usamos 'plot' de ultralytics para guardar)\n",
    "out_dir = \"runs/predict_display\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(val_imgs), figsize=(4*len(val_imgs), 4))\n",
    "if len(val_imgs) == 1:\n",
    "    axes = [axes]\n",
    "for ax, r in zip(axes, pred):\n",
    "    im = r.plot()  # numpy array con anotaciones\n",
    "    ax.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e065ca1",
   "metadata": {},
   "source": [
    "# 5. Export and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_dir = Path(\"artifacts\")\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_ckpt = None\n",
    "for p in Path(\"runs/detect\").rglob(\"weights/best.pt\"):\n",
    "    best_ckpt = p\n",
    "    break\n",
    "\n",
    "if best_ckpt and best_ckpt.exists():\n",
    "    target = export_dir / \"model_best.pt\"\n",
    "    target.write_bytes(best_ckpt.read_bytes())\n",
    "    print(\"Wheight export to:\", target)\n",
    "else:\n",
    "    print(\"'best.pt' not found\")\n",
    "\n",
    "with open(export_dir / \"classes.json\", \"w\") as f:\n",
    "    json.dump({\"nc\": int(nc), \"names\": class_names}, f, indent=2)\n",
    "\n",
    "try:\n",
    "    _ = model.export(format=\"onnx\", imgsz=IMGSZ)\n",
    "    onnx_file = None\n",
    "    for p in Path(\".\").rglob(\"*.onnx\"):\n",
    "        onnx_file = p\n",
    "        break\n",
    "    if onnx_file:\n",
    "        (export_dir / \"model.onnx\").write_bytes(onnx_file.read_bytes())\n",
    "        print(\"ONNX export to:\", export_dir / \"model.onnx\")\n",
    "except Exception as e:\n",
    "    print(\"Export ONNX not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df39bb",
   "metadata": {},
   "source": [
    "# 6. TODOs (for the candidate)\n",
    "- [ ] Analyze the class imbalance and propose strategies (weighting, augmented sampling, focal loss).\n",
    "- [ ] Tune hyperparameters (epochs, image size, augmentations) to improve mAP.\n",
    "- [ ] Record key metrics and justify the final baseline.\n",
    "- [ ] Prepare all necessary artifacts in artifacts/ for the inference service (API)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc557447",
   "metadata": {},
   "source": [
    "# 7. Appendix  Notes on relative paths\n",
    "- This notebook reads data.yaml and infers the paths to images/ and labels/ for train/, val/, and test/.\n",
    "- If you move data.yaml to another folder, adjust DATA_YAML_PATH.\n",
    "- If the dataset was downloaded from Roboflow, keep the standard YOLO folder structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
